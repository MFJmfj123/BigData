​				

# 					               求数据集最低气温

1. 环境准备：

   - 编程环境：IDEA
   - Hadoop版本：3.14
   - java版本：jdk1.8
   - 部署hadoop window环境无需在启动集群即可运行

2. 创建maven工程，导入hadoop环境依赖。项目结构如下

   - ![image-20221108202042348](https://raw.githubusercontent.com/MFJmfj123/My_File/main/imgs/202211090937723.png)
   
   
   
3. 需求：求出当天最低气温。修改内容：只取出当天最低气温，并且过滤一下脏数据（气温为9999.0或-9999.0）指定数据地址和结果输出地址。

4. 修改后的代码展示：

   ```java
   
   package com.ma;
   
   import java.io.IOException;
   import java.util.Iterator;
   
   import org.apache.hadoop.fs.Path;
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   
   import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
   import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
   import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
   import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
   import org.apache.hadoop.mapreduce.Job;
   import org.apache.hadoop.mapreduce.Mapper;
   import org.apache.hadoop.mapreduce.Reducer;
   import org.apache.hadoop.conf.Configuration;
   
   public class MyMaxMin {
   
   
       //Mapper
   
       /**
        * MaxTemperatureMapper class is static and extends Mapper abstract class
        * having four hadoop generics type LongWritable, Text, Text, Text.
        */
   
   
       public static class MaxTemperatureMapper extends
               Mapper<LongWritable, Text, Text, Text> {
   
           /**
            * @method map
            * This method takes the input as text data type.
            * Now leaving the first five tokens,it takes 6th token is taken as temp_max and
            * 7th token is taken as temp_min. Now temp_max > 35 and temp_min < 10 are passed to the reducer.
            */
   
           @Override
           public void map(LongWritable arg0, Text Value, Context context)
                   throws IOException, InterruptedException {
   
               //Converting the record (single line) to String and storing it in a String variable line
   
               String line = Value.toString();
   
               //Checking if the line is not empty
   
               if (!(line.length() == 0)) {
   
                   //date
   
                   String date = line.substring(6, 14);
   
                   float temp_Min = Float
                           .parseFloat(line.substring(47, 53).trim());
                   //不用判断是否冷天还是热天，过滤脏数据后输出
                   if (Math.abs(temp_Min) != 9999) {
   
                       context.write(new Text("MIN_temperature" + date),
                               new Text(String.valueOf(temp_Min)));
                   }
               }
           }
   
       }
   
   //Reducer
   
       
       public static class MaxTemperatureReducer extends
               Reducer<Text, Text, Text, Text> {
   
           /**
            * @method reduce
            * This method takes the input as key and list of values pair from mapper, it does aggregation
            * based on keys and produces the final context.
            */
   
           public void reduce(Text Key, Iterator<Text> Values, Context context)
                   throws IOException, InterruptedException {
   
   
               //putting all the values in temperature variable of type String
   
               String temperature = Values.next().toString();
               context.write(Key, new Text(temperature));
           }
   
       }
   
   
       /**
        * @method main
        * This method is used for setting all the configuration properties.
        * It acts as a driver for map reduce code.
        */
   
       public static void main(String[] args) throws Exception {
   
           //reads the default configuration of cluster from the configuration xml files
           Configuration conf = new Configuration();
   
           //Initializing the job with the default configuration of the cluster
           Job job = new Job(conf, "weather example");
   
           //Assigning the driver class name
           job.setJarByClass(MyMaxMin.class);
   
           //Key type coming out of mapper
           job.setMapOutputKeyClass(Text.class);
   
           //value type coming out of mapper
           job.setMapOutputValueClass(Text.class);
   
           //Defining the mapper class name
           job.setMapperClass(MaxTemperatureMapper.class);
   
           //Defining the reducer class name
           job.setReducerClass(MaxTemperatureReducer.class);
   
           //Defining input Format class which is responsible to parse the dataset into a key value pair
           job.setInputFormatClass(TextInputFormat.class);
   
           //Defining output Format class which is responsible to parse the dataset into a key value pair
           job.setOutputFormatClass(TextOutputFormat.class);
   
           //setting the second argument as a path in a path variable
           //指定本地输出路径
           Path OutputPath = new Path("M:\\data\\output");
   
           //Configuring the input path from the filesystem into the job
           //指定数据集位置
           FileInputFormat.addInputPath(job, new Path("M:\\data\\CRND0103-2022-VA_Charlottesville_2_SSE.txt"));
   
           //Configuring the output path from the filesystem into the job
           FileOutputFormat.setOutputPath(job, new Path("M:\\data\\output"));
   
           //deleting the context path automatically from hdfs so that we don't have delete it explicitly
           OutputPath.getFileSystem(conf).delete(OutputPath);
   
           //exiting the job only if the flag value becomes false
           System.exit(job.waitForCompletion(true) ? 0 : 1);
   
       }
   }
   ```

5. 打包运行maven工程，运行无错误在指定路径下查看结果集，展示部分。

   - ![image-20221107204316121](https://raw.githubusercontent.com/MFJmfj123/My_File/main/imgs/202211090938546.png)
   - ![image-20221107204403057](https://raw.githubusercontent.com/MFJmfj123/My_File/main/imgs/202211090938073.png)

6. 需求：求出全年最低气温

   - 代码思路：拿出每天最低气温，在reduce阶段进行迭代筛选出最低气温。

   - 代码如下

     ```java
     package com.ma;
     
     import java.io.IOException;
     
     import org.apache.hadoop.fs.Path;
     import org.apache.hadoop.io.DoubleWritable;
     import org.apache.hadoop.io.LongWritable;
     import org.apache.hadoop.io.Text;
     
     import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
     import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
     import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
     import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
     import org.apache.hadoop.mapreduce.Job;
     import org.apache.hadoop.mapreduce.Mapper;
     import org.apache.hadoop.mapreduce.Reducer;
     import org.apache.hadoop.conf.Configuration;
     
     public class MyMaxMin {
         //Mapper
         public static class MaxTemperatureMapper extends
                 Mapper<LongWritable, Text, Text, DoubleWritable> {
             @Override
             public void map(LongWritable arg0, Text Value, Context context) throws IOException, InterruptedException {
                 String line = Value.toString();
                 if (!(line.length() == 0)) {
                     String date = line.substring(6, 14);
                     Double temp_Min = Double.parseDouble(line.substring(47, 53).trim());
                     if (Math.abs(temp_Min) != 9999) {
                         context.write(new Text("MIN_temperature"),
                                 new DoubleWritable(temp_Min));
                     }
                 }
             }
     
         }
     	//迭代筛选出最低气温
         //Reducer
         public static class MaxTemperatureReducer extends
                 Reducer<Text, DoubleWritable, Text, DoubleWritable> {
             @Override
             protected void reduce(Text key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {
                 double min = Double.MAX_VALUE;
                 for (DoubleWritable value : values) {
     
                     double tmp = value.get();
                     if (tmp < min) {
                         min = tmp;
                     }
     
                 }
                 context.write(key, new DoubleWritable(min));
             }
         }
     
         public static void main(String[] args) throws Exception {
             //reads the default configuration of cluster from the configuration xml files
     
             Configuration conf = new Configuration();
     
             //Initializing the job with the default configuration of the cluster
             Job job = new Job(conf, "weather example");
     
             //Assigning the driver class name
             job.setJarByClass(MyMaxMin.class);
     
             //Key type coming out of mapper
             job.setMapOutputKeyClass(Text.class);
     
             //value type coming out of mapper
             job.setMapOutputValueClass(Text.class);
             job.setMapOutputValueClass(DoubleWritable.class);
     
             //Defining the mapper class name
             job.setMapperClass(MaxTemperatureMapper.class);
             //Defining the reducer class name
             job.setReducerClass(MaxTemperatureReducer.class);
     
             job.setOutputValueClass(DoubleWritable.class);
             //Defining input Format class which is responsible to parse the dataset into a key value pair
             job.setInputFormatClass(TextInputFormat.class);
     
             //Defining output Format class which is responsible to parse the dataset into a key value pair
             job.setOutputFormatClass(TextOutputFormat.class);
     
             //setting the second argument as a path in a path variable
             //指定本地输出路径
             Path OutputPath = new Path("M:\\data\\output");
     
             //Configuring the input path from the filesystem into the job
             //指定数据集位置
             FileInputFormat.addInputPath(job, new Path("M:\\data\\CRND0103-2022-VA_Charlottesville_2_SSE.txt"));
     
             //Configuring the output path from the filesystem into the job
             FileOutputFormat.setOutputPath(job, new Path("M:\\data\\output"));
     
             //deleting the context path automatically from hdfs so that we don't have delete it explicitly
             OutputPath.getFileSystem(conf).delete(OutputPath);
     
             //exiting the job only
           
            	if the flag value becomes false
             System.exit(job.waitForCompletion(true) ? 0 : 1);
     
         }
     }
     ```

   - 查看结果集
     - ![image-20221108204029720](https://raw.githubusercontent.com/MFJmfj123/My_File/main/imgs/202211090938986.png)
     - ![image-20221108204139391](https://raw.githubusercontent.com/MFJmfj123/My_File/main/imgs/202211090938548.png)